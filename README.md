# L5-DE-Data-Pipelines-T4

This project develops and expands a user data pipeline for a growing international application. It integrates user metadata and login records from the UK, France, the US, and now Scotland! The pipeline is designed to standardise diverse data formats, enforce validation rules, and support scalable ingestion into a unified SQLite database. It also now modularises the code, and adds elements of testing.

## ðŸš€ Project Overview

Your objectives now are listed below:
   - Update and expand the database schema to support multi-country user data.
   - Clean, validate, and integrate data from UK, French, US, and SC sources.
   - Handle local formatting differences (e.g. salary, date, gender, encoding).
   - Maintain a shared schema with country-specific data pipelines.
   - Prepare for future global integrations with additional countries.


**Note: Collaborate with Your Instructor**  
 Your instructor will act as the stakeholder and Subject Matter Expert (SME). They will provide clarification on:
   - Schema requirements
   - Business rules for validation
   - Future data sources

---

## ðŸ”„ Data Pipeline Enhancements
The pipeline should been updated to process UK, FR, US, and SC datasets:
- Config files allow defining of parameters spcific to each file type.
- Clean and normalise data fields (e.g., DoB formats, salary parsing, gender codes).
- Add the appropriate 'country_code' (e.g., 'UK', 'FR') for each user entry.
- Merge data into a shared 'users' and 'logins' table with consistent structure.

---

## ðŸ“¦ Repository Structure

```plaintext
â”œâ”€â”€ config/                         # JSON configs for mappings and exclusions
â”‚   â”œâ”€â”€ exclusions.json
â”‚   â”œâ”€â”€ mappings_uk.json
â”‚   â”œâ”€â”€ mappings_fr.json
â”‚   â”œâ”€â”€ mappings_usa.json
|   â””â”€â”€ mappings_sc.json
â”‚
â”œâ”€â”€ data/                           # Raw CSV data files
â”‚   â”œâ”€â”€ FR User Data.csv
â”‚   â”œâ”€â”€ FR-User-LoginTS.csv
â”‚   â”œâ”€â”€ SC User Data.csv
â”‚   â”œâ”€â”€ SC-User-LoginTS.csv
â”‚   â”œâ”€â”€ UK User Data.csv
â”‚   â”œâ”€â”€ UK-User-LoginTS.csv
â”‚   â”œâ”€â”€ USA User Data.csv
â”‚   â””â”€â”€ USA-User-LoginTS.csv
â”‚
â”œâ”€â”€ pipeline/                       # Python package for reusable pipeline code
â”‚   â”œâ”€â”€ __init__.py                 # Empty file to make this a package
â”‚   â”œâ”€â”€ config_loader.py
â”‚   â”œâ”€â”€ data_utils.py               # utility functions (mostly for transforming data)
â”‚   â”œâ”€â”€ database.py                 # Python code related to database management
â”‚   â””â”€â”€ transform.py                # workhorse of transformations
â”‚
â”œâ”€â”€ sql/                            # SQL schema script
â”‚   â””â”€â”€ create_database.sql
â”‚
â”œâ”€â”€ tests/                          # Test coverage
â”‚   â”œâ”€â”€ test_pipeline.py            # testing for issues with code
â”‚   â””â”€â”€ validation.py               # validating data
â”‚
â”œâ”€â”€ data_pipeline.py                # Starting point for your script
â”œâ”€â”€ main.py                         # Main orchestration script
â”œâ”€â”€ requirements.txt                # Optional dependency list
â”œâ”€â”€ README.md                       # Project documentation
```

You will work from `data_pipeline.py`, and create the modularised content in `config`, `data`, `pipeline`, `sql`, `tests` and `main.py`.

---

## ðŸ§  Remember

This setup uses **SQLite** for simplicity and local prototyping, but the same schema and logic should be portable to PostgreSQL or another RDBMS in a production environment. Keep modularity and maintainability in mind.

Happy coding!